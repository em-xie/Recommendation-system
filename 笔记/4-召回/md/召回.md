[(78条消息) Jupyter-lab环境配置/调试远程服务器代码_jupyter lab --ip_牧之原的博客-CSDN博客](https://blog.csdn.net/Mr_TangGuo/article/details/110633387?spm=1001.2014.3001.5501)

# 筛选用户喜欢的

![image-20230507120035565](E:\PyCode\Recommendation-system\笔记\4-召回\md\image-20230507120035565.png)

# word2vec整合进推荐系统

![image-20230507121345779](E:\PyCode\Recommendation-system\笔记\4-召回\md\image-20230507121345779.png)

![image-20230507121524117](E:\PyCode\Recommendation-system\笔记\4-召回\md\image-20230507121524117.png)

![image-20230507121901796](E:\PyCode\Recommendation-system\笔记\4-召回\md\image-20230507121901796.png)

![image-20230507121959494](E:\PyCode\Recommendation-system\笔记\4-召回\md\image-20230507121959494.png)

![image-20230507152226147](E:\PyCode\Recommendation-system\笔记\4-召回\md\image-20230507152226147.png)

![image-20230507152247638](E:\PyCode\Recommendation-system\笔记\4-召回\md\image-20230507152247638.png)

![image-20230507152420809](E:\PyCode\Recommendation-system\笔记\4-召回\md\image-20230507152420809.png)

![image-20230507152436427](E:\PyCode\Recommendation-system\笔记\4-召回\md\image-20230507152436427.png)

![image-20230507152635960](E:\PyCode\Recommendation-system\笔记\4-召回\md\image-20230507152635960.png)



# 实现item2vec- deepwalk+wd2vec

```
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.functions import col, collect_list
from collections import defaultdict
import numpy as np
from pyspark.sql.session import SparkSession
from recall.config import config
import dill
import os.path

rng = np.random.default_rng()


def build_seq(rating_df: DataFrame, spark: SparkSession):
    entrance_items = None
    entrance_probs = None
    transfer_probs = None
    # deep walk 从缓存中读取 增量更新
    if os.path.isfile('output/entrance_items.dill'):
        with open('output/entrance_items.dill', 'rb') as f:
            entrance_items = dill.load(f)
        with open('output/entrance_probs.dill', 'rb') as f:
            entrance_probs = dill.load(f)
        with open('output/transfer_probs.dill', 'rb') as f:
            transfer_probs = dill.load(f)
        print('loaded model from file')
    else:
        # 正常deep walk 流程
        rating_df = rating_df.where('rating > 7')
        # 1/group by user_id

        watch_seq_df = rating_df \
            .groupBy('user_id') \
            .agg(
                collect_list(col('anime_id').cast('string')).alias('anime_ids')
            )
        # 2.build anime matrix 邻接矩阵
        watch_seq = watch_seq_df.collect()
        # 处理号的结果进行转换 2列 -> 取每一行的anime_ids
        watch_seq = [s['anime_ids'] for s in watch_seq]
        # 邻接矩阵 2维 默认值是0
        matrix = defaultdict(lambda: defaultdict(int))
        # 赋值
        for i in range(len(watch_seq)):
            if i % 1000 == 0:
                print(f'matrix add seq {i}')
            seq = watch_seq[i]
            add_seq_to_matrix(seq, matrix)
        # 3.build prob transfer matrix 概率转移矩阵
        transfer_probs = {k: get_transfer_prob(v) for k, v in matrix.items()}
        print(f'transfer probs built. {len(transfer_probs)} entrances')

        # 4.entrance 入口构造
        counts = {k: sum(neibors.values()) for k, neibors in matrix.items()}
        entrance_items = list(transfer_probs.keys())
        total_count = sum(counts.values())
        entrance_probs = [counts[k] / total_count for k in entrance_items]
        print('entrance probs built.')

        with open('output/entrance_items.dill', 'wb') as f:
            dill.dump(entrance_items, f)
        with open('output/entrance_probs.dill', 'wb') as f:
            dill.dump(entrance_probs, f)
        with open('output/transfer_probs.dill', 'wb') as f:
            dill.dump(transfer_probs, f)
        print('saved model to file')

    # 5.do random walk
    n = config['deepwalk']['sample_count']
    length = config['deepwalk']['sample_length']
    samples = []
    for i in range(n):
        if i % 1000 == 0:
            print(f'random walk done {i}')
        s = one_random_walk(length, entrance_items, entrance_probs, transfer_probs)
        samples.append(s)
    print(f'Random walk generated {n} sample.')

    return spark.createDataFrame([[row] for row in samples], ['anime_ids'])


def add_seq_to_matrix(seq, m):
    for i in range(len(seq)):
        for j in range(i + 1, len(seq)):
            a = seq[i]
            b = seq[j]
            if a == b:
                continue
            m[a][b] += 1
            m[b][a] += 1


def get_transfer_prob(v):
    # 计算转移概率
    neighbours = v.keys()
    total_weight = sum(v.values())
    probs = [v[k] / total_weight for k in neighbours]
    return {
        'neighbours': neighbours,
        'probs': probs
    }


def one_random_walk(length, entrance_items, entrance_probs, transfer_probs):
    start_point = rng.choice(entrance_items, 1, p=entrance_probs)[0]
    path = [str(start_point)]
    current_point = start_point
    for _ in range(length):
        neighbours = transfer_probs[current_point]['neighbours']
        transfer_probs = transfer_probs[current_point]['probs']
        next_point = rng.choice(neighbours, 1, p=transfer_probs)[0]
        path.append(str(next_point))
        current_point = next_point

    return path

```

```
from typing import Tuple
from pyspark.sql.dataframe import DataFrame
from pyspark.sql.functions import col, collect_list, udf
import numpy as np
from recall.config import config
from pyspark.ml.feature import Word2Vec


def train_item2vec(anime_seq: DataFrame, rating_df: DataFrame) -> Tuple[DataFrame, DataFrame]:
    model_config = config['item2vec']
    # spark训练 维度 反复训练几次 窗口大小
    word2vec = Word2Vec(vectorSize=model_config['vector_size'], maxIter=model_config['max_iter']
                        , windowSize=model_config['windowSize'])
    word2vec.setInputCol('anime_ids')
    word2vec.setOutputCol('anime_ids_vec')
    model = word2vec.fit(anime_seq)
    item_emb_df = model.getVectors()
    # embedding
    item_vec = model.getVectors().collect()
    item_emb = {}
    # 转成字典
    for item in item_vec:
        item_emb[item.word] = item.vector.toArray()

    # 传回
    @udf(returnType='array<float>')
    def build_user_emb(anime_seq):
        # id映射 anime_seq中的id到anime_embs
        anime_embs = [item_emb[aid] if aid in item_emb else [] for aid in anime_seq]
        # 求均值 过滤
        anime_embs = list(filter(lambda l: len(l) > 0, anime_embs))
        ret = np.mean(anime_embs, axis=0).tolist()

        return ret

    user_emb_df = rating_df \
        .where('rating > 7') \
        .groupBy('user_id') \
        .agg(
        collect_list(col('anime_id').cast('string')).alias('anime_ids')
    ) \
        .withColumn('user_emb', build_user_emb(col('anime_ids')))

    return (
        item_emb_df,
        user_emb_df
    )

```

# embedding - redis

```
from typing import Dict, List
from redis import Redis
from recall.config import config

redis_config = config['redis']
redis = Redis(host=redis_config['host'], port=redis_config['port'], db=redis_config['db'], password=redis_config['pwd'])
ITEM_EMB_KEY = 'recall:emb:item'
USER_EMB_KEY = 'recall:emb:user'


def save_item_embedding(item_emb: Dict):
    encoded_emb = {k: stringify_vector(v) for (k, v) in item_emb.items()}
    redis.hset(ITEM_EMB_KEY, mapping=encoded_emb)


def save_user_embedding(user_emb):
    encoded_emb = {k: stringify_vector(v) for (k, v) in user_emb.items()}
    redis.hset(USER_EMB_KEY, mapping=encoded_emb)


def stringify_vector(vec):
    if vec is None:
        return ''
    return ':'.join(list(map(lambda v: str(v), vec)))


def str2vec(s):
    if len(s) == 0:
        return [float(x) for x in s.split(':')]


def get_all_item_embedding() -> Dict[int, List[float]]:
    data = redis.hgetall(ITEM_EMB_KEY)
    res = {int(k.decode()): parser_vector_string(v.decode()) for (k, v) in data.items()}
    return res


def get_all_user_embedding() -> Dict[int, List[float]]:
    data = redis.hgetall(USER_EMB_KEY)
    res = {int(k.decode()): parser_vector_string(v.decode()) for (k, v) in data.items()}
    return res


def parser_vector_string(s):
    if len(s) == 0:
        return None
    return [float(x) for x in s.split(':')]


def get_one_item_embedding(item_id: int) -> List[float]:
    emb = redis.hget(ITEM_EMB_KEY, item_id)
    if emb is None:
        return None
    return parser_vector_string(emb.decode())


def get_one_user_embedding(user_id: int) -> List[float]:
    emb = redis.hget(USER_EMB_KEY, user_id)
    if emb is None:
        return None
    return parser_vector_string(emb.decode())

```

# lsh

```
from typing import Dict,List
import numpy as np
import faiss
from recall.dataset.embedding import get_one_user_embedding,get_all_item_embedding

class LSH:
    def __init__(self,embedding:Dict[int,List[float]]) ->None:
        # 分成一个元组
        items = embedding.items()
        # 第一个元素 id
        self.ids = [i[0]for i in items]
        #第二个元素 embedding
        vectors = [i[1] for i in items]

        d = len(vectors[0])
        print(f'd={d}')
        # 建立索引  向量维度embedding的长度 平面切割256
        self.index = faiss.IndexLSH(d,256)
        # 转化成向量
        array_vec = np.asarray(vectors,dtype=np.float32)
        # 加入训练
        self.index.add(array_vec)
        assert(self.index.is_trained)
        print(f'LSH index added {self.index.ntotal} vectors')

    def search(self,vec:List[float],n=20) ->List[int]:
        print(vec)
        # 下标检索 2个向量返回
        D,I = self.index.search(np.asarray([vec],dtype=np.float32),n)
        neighbors = I[0]
        res = [self.ids[i] for i in neighbors]
        print('D:')
        print(D)


def get_item_lsh():
    return None
```

# 策略

```
from typing import List
from recall.strategy.recall_strategy import RecallStrategy
from recall.model.lsh import get_item_lsh
from recall.dataset.embedding import get_one_user_embedding, get_all_item_embedding
from recall.context import Context

class UserEmbeddingStrategy(RecallStrategy):
    def __init__(self) -> None:
        super().__init__()
        self.lsh = get_item_lsh()


    def name(self):
        return 'UserEmbedding'

    def recall(self, context: Context, n) -> List[int]:
        if context.user_id is None:
            return []
        user_id = context.user_id
        user_emb = get_one_user_embedding(user_id)
        if user_emb is None:
            return []
        return self.lsh.search(user_emb,n = n)


```

```
from recall.strategy.recall_strategy import RecallStrategy
import recall.dataset.anime as dataset
from recall.context import Context
from typing import List

(anime_df, _) = dataset.load_dataset()
sorted_df = anime_df.sort_index()


class SimilarAnimeStrategy(RecallStrategy):

    def name(self):
        return 'Simple Anime'

    def recall(self, context: Context, n=20) -> List[int]:
        # 返回和目标动漫id相近的结果
        anime_ioc = sorted_df.index.get_loc(context.anime_id)
        from_index = anime_ioc
        if from_index + n > len(sorted_df):
            from_index = len(sorted_df) - n

        return sorted_df.iloc[from_index: from_index + n].index.to_list()

```

```
# import os
#
# config = {
#     'dataset_path': os.environ['DATASET_PATH']
# }
# def config():
#     return None

config = {
    'redis': {
        'host': 'http://127.0.0.1',
        'port': 6379,
        'db': 'anime',
        'pwd': 123456
    },
    'item2vec':{
        'vector_size': 2,
        'max_iter': 2,
        'windowSize': 1
    },
    'deepwalk''sample_count':{
        5
    },
    'deepwalk''sample_length':{
        5
    }
}
```

